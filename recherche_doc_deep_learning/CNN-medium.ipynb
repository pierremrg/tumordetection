{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilisation du réseau de neurone décrit : https://medium.com/@mohamedalihabib7/brain-tumor-detection-using-convolutional-neural-networks-30ccef6612b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_img = []\n",
    "labels = []\n",
    "\n",
    "n_y = 0\n",
    "n_n = 0\n",
    "#print(os.getcwd())\n",
    "directory = '../../new_data_norm2/'\n",
    "for name in os.listdir(directory + 'yes'):\n",
    "    img = Image.open(directory + 'yes/'+ name)\n",
    "    img = img.resize((240,240))\n",
    "    list_img.append(np.asarray(img).T)\n",
    "    labels.append(1)\n",
    "    n_y += 1\n",
    "    \n",
    "for name in os.listdir(directory + '/no'):\n",
    "    img = Image.open(directory + 'no/'+ name)\n",
    "    img = img.resize((240,240))\n",
    "    list_img.append(np.asarray(img).T)\n",
    "    labels.append(0)\n",
    "    n_n += 1\n",
    "    \n",
    "#print(n_y, n_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 240, 240)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_img[4].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class brain_Dataset(Dataset):\n",
    "    def __init__ (self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data[1])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[0][index]\n",
    "        label = self.data[1][index]\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = brain_Dataset([list_img, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "\n",
    "data_size = len(list_img)\n",
    "validation_split = .2\n",
    "split = int(np.floor(validation_split * data_size))\n",
    "indices = list(range(data_size))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "valloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class brainCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(brainCNN, self).__init__()\n",
    "        \n",
    "        self.zp = nn.ConstantPad2d(2, 0)\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=(7,7)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.mp1 = nn.MaxPool2d(4, stride=4)\n",
    "        self.mp2 = nn.MaxPool2d(4, stride=4)\n",
    "        self.ft = nn.Flatten()\n",
    "        \n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(6272, 2),\n",
    "            nn.ReLU())\n",
    "        self.sig = nn.Sigmoid()        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.zp(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.mp1(x)\n",
    "        x = self.mp2(x)\n",
    "        x = self.ft(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.sig(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement\n",
    "\n",
    "##### Obtention de bon résultat avec beaucoup d'epochs (~20) sur les données augmentés et crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            x_gpu = x.to(device, dtype=torch.float)\n",
    "            y_gpu = y.to(device, dtype=torch.long)\n",
    "            prediction = model(x_gpu)   \n",
    "            loss_value = loss(prediction, y_gpu)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "\n",
    "            correct_samples += torch.sum(indices == y_gpu)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / i_step\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy = compute_accuracy(model, val_loader)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        \n",
    "        print(\"Average loss: %f, Train accuracy: %f, Val accuracy: %f\" % (ave_loss, train_accuracy, val_accuracy))\n",
    "        \n",
    "    return loss_history, train_history, val_history\n",
    "        \n",
    "def compute_accuracy(model, loader):\n",
    "    model.eval() # Evaluation mode\n",
    "    \n",
    "    correct_samples = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for i_step, (x, y) in enumerate(loader):\n",
    "      x_gpu = x.to(device, dtype=torch.float)\n",
    "      y_gpu = y.to(device, dtype=torch.long)\n",
    "      predictions = model(x_gpu)\n",
    "      _, indices = torch.max(predictions, 1)\n",
    "      correct_samples += torch.sum(indices == y_gpu)\n",
    "      total_samples += y.shape[0]\n",
    "      \n",
    "    accuracy = float(correct_samples)/total_samples       \n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = brainCNN()\n",
    "model.type(torch.cuda.FloatTensor)\n",
    "model.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr =1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.496509, Train accuracy: 0.980843, Val accuracy: 0.864450\n",
      "Average loss: 0.497067, Train accuracy: 0.977650, Val accuracy: 0.859335\n",
      "Average loss: 0.496537, Train accuracy: 0.980843, Val accuracy: 0.843990\n",
      "Average loss: 0.498109, Train accuracy: 0.978927, Val accuracy: 0.861893\n",
      "Average loss: 0.496840, Train accuracy: 0.978927, Val accuracy: 0.864450\n",
      "Average loss: 0.496947, Train accuracy: 0.979566, Val accuracy: 0.859335\n",
      "Average loss: 0.499146, Train accuracy: 0.980204, Val accuracy: 0.864450\n",
      "Average loss: 0.496927, Train accuracy: 0.979566, Val accuracy: 0.867008\n",
      "Average loss: 0.497590, Train accuracy: 0.981481, Val accuracy: 0.841432\n",
      "Average loss: 0.498418, Train accuracy: 0.978289, Val accuracy: 0.854220\n",
      "Average loss: 0.499922, Train accuracy: 0.980204, Val accuracy: 0.849105\n",
      "Average loss: 0.495687, Train accuracy: 0.980204, Val accuracy: 0.856777\n",
      "Average loss: 0.496399, Train accuracy: 0.979566, Val accuracy: 0.851662\n",
      "Average loss: 0.495169, Train accuracy: 0.980843, Val accuracy: 0.877238\n",
      "Average loss: 0.494017, Train accuracy: 0.982120, Val accuracy: 0.872123\n",
      "Average loss: 0.496149, Train accuracy: 0.980843, Val accuracy: 0.856777\n",
      "Average loss: 0.496559, Train accuracy: 0.982120, Val accuracy: 0.861893\n",
      "Average loss: 0.495609, Train accuracy: 0.980843, Val accuracy: 0.856777\n",
      "Average loss: 0.495431, Train accuracy: 0.978927, Val accuracy: 0.872123\n",
      "Average loss: 0.493780, Train accuracy: 0.980204, Val accuracy: 0.861893\n",
      "Average loss: 0.494655, Train accuracy: 0.979566, Val accuracy: 0.859335\n",
      "Average loss: 0.494159, Train accuracy: 0.980204, Val accuracy: 0.872123\n",
      "Average loss: 0.494833, Train accuracy: 0.981481, Val accuracy: 0.874680\n",
      "Average loss: 0.495493, Train accuracy: 0.982120, Val accuracy: 0.864450\n",
      "Average loss: 0.494623, Train accuracy: 0.982759, Val accuracy: 0.854220\n",
      "Average loss: 0.493480, Train accuracy: 0.981481, Val accuracy: 0.854220\n",
      "Average loss: 0.493865, Train accuracy: 0.984036, Val accuracy: 0.864450\n",
      "Average loss: 0.495320, Train accuracy: 0.982120, Val accuracy: 0.843990\n",
      "Average loss: 0.494050, Train accuracy: 0.983397, Val accuracy: 0.867008\n",
      "Average loss: 0.493551, Train accuracy: 0.980204, Val accuracy: 0.859335\n",
      "Average loss: 0.493692, Train accuracy: 0.982759, Val accuracy: 0.861893\n",
      "Average loss: 0.493504, Train accuracy: 0.982759, Val accuracy: 0.872123\n",
      "Average loss: 0.493098, Train accuracy: 0.983397, Val accuracy: 0.869565\n",
      "Average loss: 0.492590, Train accuracy: 0.982759, Val accuracy: 0.859335\n",
      "Average loss: 0.492390, Train accuracy: 0.982759, Val accuracy: 0.846547\n",
      "Average loss: 0.494440, Train accuracy: 0.981481, Val accuracy: 0.872123\n",
      "Average loss: 0.492816, Train accuracy: 0.982120, Val accuracy: 0.869565\n",
      "Average loss: 0.492814, Train accuracy: 0.984036, Val accuracy: 0.856777\n",
      "Average loss: 0.494298, Train accuracy: 0.984036, Val accuracy: 0.859335\n",
      "Average loss: 0.493002, Train accuracy: 0.984036, Val accuracy: 0.856777\n",
      "Average loss: 0.493261, Train accuracy: 0.982759, Val accuracy: 0.859335\n",
      "Average loss: 0.492296, Train accuracy: 0.984036, Val accuracy: 0.856777\n",
      "Average loss: 0.493297, Train accuracy: 0.983397, Val accuracy: 0.846547\n",
      "Average loss: 0.492891, Train accuracy: 0.985951, Val accuracy: 0.861893\n",
      "Average loss: 0.494995, Train accuracy: 0.983397, Val accuracy: 0.856777\n",
      "Average loss: 0.492652, Train accuracy: 0.984036, Val accuracy: 0.836317\n",
      "Average loss: 0.493700, Train accuracy: 0.983397, Val accuracy: 0.867008\n",
      "Average loss: 0.491633, Train accuracy: 0.983397, Val accuracy: 0.869565\n",
      "Average loss: 0.491774, Train accuracy: 0.984036, Val accuracy: 0.859335\n",
      "Average loss: 0.493423, Train accuracy: 0.985313, Val accuracy: 0.872123\n"
     ]
    }
   ],
   "source": [
    "loss_history, train_history, val_history = train_model(model, trainloader, valloader, loss, optimizer, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
